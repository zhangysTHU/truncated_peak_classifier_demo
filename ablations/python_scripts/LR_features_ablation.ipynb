{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python import\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn import svm\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_kftest(X_train, y_train, X_test, y_test, SEED):    \n",
    "    # Logistic Regression params\n",
    "    lr_param_dict = {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [1e-3, 5e-3, 1e-2, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000],\n",
    "        \"solver\": [\"liblinear\"],\n",
    "        \"random_state\": [SEED]\n",
    "    }\n",
    "\n",
    "    # Initiate Logistic Regression model\n",
    "    lr_model = LogisticRegression()\n",
    "\n",
    "    # Adjust hyper-parameters with randomized search\n",
    "    lr_rscv = RandomizedSearchCV(lr_model, lr_param_dict, n_iter=100, cv=5, verbose=0,\n",
    "                                 scoring=\"roc_auc\", random_state=SEED, n_jobs=-1)\n",
    "    lr_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = lr_rscv.best_estimator_.predict(X_test)\n",
    "    y_pred_proba = lr_rscv.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    aurp = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "    return acc, auroc, f1, aurp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output result of evaluation\n",
    "def eval_output(model_perf,path):\n",
    "    with open(os.path.join(path,\"Evaluate_Result_TestSet.txt\"),'w') as f:\n",
    "        f.write(\"AUROC=%s\\tAUPRC=%s\\tAccuracy=%s\\tMCC=%s\\tRecall=%s\\tPrecision=%s\\tf1_score=%s\\n\" %\n",
    "               (model_perf[\"auroc\"],model_perf[\"auprc\"],model_perf[\"accuracy\"],model_perf[\"mcc\"],model_perf[\"recall\"],model_perf[\"precision\"],model_perf[\"f1\"]))\n",
    "        f.write(\"\\n######NOTE#######\\n\")\n",
    "        f.write(\"#According to help_documentation of sklearn.metrics.classification_report:in binary classification, recall of the positive class is also known as sensitivity; recall of the negative class is specificity#\\n\\n\")\n",
    "        f.write(model_perf[\"class_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_kind(data_df):\n",
    "    df_y0 = data_df[data_df['label'] == 0]\n",
    "    df_y1 = data_df[data_df['label'] == 1]\n",
    "\n",
    "    # 确定两个子集中数量较少的那个\n",
    "    min_count = min(len(df_y0), len(df_y1))\n",
    "\n",
    "    # 从两个子集中随机选择等量的样本\n",
    "    df_y0_balanced = df_y0.sample(n=min_count, random_state=42) if len(df_y0) > min_count else df_y0\n",
    "    df_y1_balanced = df_y1.sample(n=min_count, random_state=42) if len(df_y1) > min_count else df_y1\n",
    "\n",
    "    # 合并这两个平衡后的子集\n",
    "    balanced_df = pd.concat([df_y0_balanced, df_y1_balanced])\n",
    "    # 打乱合并后的数据集的顺序\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b不进行测试集划分，直接合并所有文件\n",
    "def prepare_full_dataset(label_folder_path, sample_folder_path, features_num):\n",
    "    \"\"\"\n",
    "    直接将所有样本数据和标签合并为一个 dataset，并返回特征和标签。\n",
    "    \n",
    "    :param label_folder_path: 标签文件夹路径\n",
    "    :param sample_folder_path: 样本文件夹路径\n",
    "    :param features_num: 用于训练的特征数量\n",
    "    :return: 特征和标签的 numpy 数组\n",
    "    \"\"\"\n",
    "    # 获取所有 CSV 文件的文件名，并按文件名排序\n",
    "    label_csv_files = sorted([f for f in os.listdir(label_folder_path) if f.endswith('.csv')])\n",
    "    sample_csv_files = sorted([f for f in os.listdir(sample_folder_path) if f.endswith('.csv')])\n",
    "\n",
    "    # 用于存储所有标签和样本的 DataFrame\n",
    "    all_labels = []\n",
    "    all_samples = []\n",
    "\n",
    "    # 加载所有标签和样本文件\n",
    "    for label_file, sample_file in zip(label_csv_files, sample_csv_files):\n",
    "        all_labels.append(pd.read_csv(os.path.join(label_folder_path, label_file)))\n",
    "        all_samples.append(pd.read_csv(os.path.join(sample_folder_path, sample_file)))\n",
    "\n",
    "    # 合并所有标签和样本\n",
    "    all_labels_df = pd.concat(all_labels, axis=0)\n",
    "    all_samples_df = pd.concat(all_samples, axis=0)\n",
    "\n",
    "    # 合并样本和标签数据\n",
    "    merged_df = pd.merge(all_samples_df, all_labels_df, on='sample', how='left')\n",
    "    \n",
    "    # 调用 equal_kind 函数处理合并后的数据\n",
    "    merged_df = equal_kind(merged_df)\n",
    "\n",
    "    # 提取特征和标签\n",
    "    features = merged_df.iloc[:, 1:features_num + 1].values  # 提取特征列\n",
    "    labels = merged_df.iloc[:, -1].values  # 提取标签列\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUROC of model\n",
    "def plot_AUROC(model_perf,path):\n",
    "    #get AUROC,FPR,TPR and threshold\n",
    "    roc_auc = model_perf[\"auroc\"]\n",
    "    fpr,tpr,threshold = model_perf[\"auroc_curve\"]\n",
    "    #return AUROC info\n",
    "    temp_df = pd.DataFrame({\"FPR\":fpr,\"TPR\":tpr})\n",
    "    temp_df.to_csv(os.path.join(path,\"AUROC_info.txt\"),header = True,index = False, sep = '\\t')\n",
    "    #plot\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='AUROC (area = %0.2f)' % roc_auc) \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"AUROC of Models\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(path,\"AUROC_TestSet.pdf\"),format = \"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "SEED = 100\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with features_num = 640\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 600\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 560\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 520\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 480\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 440\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 400\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 360\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 320\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 280\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 240\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 200\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 160\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 120\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 80\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 40\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Experiment completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "label_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/labels/'\n",
    "sample_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/samples/'\n",
    "# 定义不同的 features_num\n",
    "features_nums = [640, 600, 560, 520, 480, 440, 400, 360, 320, 280, 240, 200, 160, 120, 80, 40]\n",
    "# features_nums = [600]\n",
    "# 结果记录\n",
    "all_results = []\n",
    "\n",
    "# 进行循环\n",
    "for features_num in features_nums:\n",
    "    print(f\"Running with features_num = {features_num}\")\n",
    "    \n",
    "    # 准备数据集\n",
    "    X, y = prepare_full_dataset(label_folder_path, sample_folder_path, features_num=features_num)\n",
    "    \n",
    "    # 五折交叉验证\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_auroc = []\n",
    "    fold_F1 = []\n",
    "    fold_aurp = []\n",
    "\n",
    "    # 五折交叉验证\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        \n",
    "        # 获取当前折的训练集和验证集\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "\n",
    "        # 在当前折上进行训练和验证\n",
    "        acc, auroc, F1, aurp = LR_kftest(X_train, y_train, X_val, y_val, SEED)\n",
    "        \n",
    "        fold_accuracies.append(acc)\n",
    "        fold_auroc.append(auroc)\n",
    "        fold_F1.append(F1)\n",
    "        fold_aurp.append(aurp)\n",
    "\n",
    "    # 计算五折的平均值和方差\n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    acc_variance = np.var(fold_accuracies)\n",
    "    mean_auroc = np.mean(fold_auroc)\n",
    "    auroc_variance = np.var(fold_auroc)\n",
    "    mean_F1  = np.mean(fold_F1)\n",
    "    F1_variance = np.var(fold_F1)\n",
    "    mean_aurp = np.mean(fold_aurp)\n",
    "    aurp_variance = np.var(fold_aurp)\n",
    "\n",
    "    # 设置当前实验的结果\n",
    "    results = {\n",
    "        \"features_num\": features_num,\n",
    "        \"mean_accuracy\": mean_acc,\n",
    "        \"accuracy_variance\": acc_variance,\n",
    "        \"mean_auroc\": mean_auroc,\n",
    "        \"auroc_variance\": auroc_variance,\n",
    "        \"mean_F1\": mean_F1,\n",
    "        \"F1_variance\": F1_variance,\n",
    "        \"mean_aurp\": mean_aurp,\n",
    "        \"aurp_variance\": aurp_variance\n",
    "    }\n",
    "\n",
    "    # 将当前实验的结果添加到结果列表中\n",
    "    all_results.append(results)\n",
    "\n",
    "# 结果保存路径\n",
    "csv_file = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/ML_models/eight_sample_11features_test/5fold_features_ablation/LR_ablation_results.csv'\n",
    "\n",
    "# 检查文件是否存在\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# 打开文件并追加结果\n",
    "with open(csv_file, mode='a', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=results.keys())\n",
    "    \n",
    "    # 如果文件不存在，写入标题\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # 写入每次实验的结果\n",
    "    for result in all_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(\"Experiment completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with features_num = 640\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 600\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 560\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 520\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 480\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 440\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 400\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 360\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 320\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 280\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 240\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 200\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 160\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 120\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 80\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 40\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Experiment completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# 将所有特征顺序倒序消融，先消除coverage\n",
    "\n",
    "label_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/labels/'\n",
    "sample_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/samples/'\n",
    "# 定义不同的 features_num\n",
    "features_nums = [640, 600, 560, 520, 480, 440, 400, 360, 320, 280, 240, 200, 160, 120, 80, 40]\n",
    "# features_nums = [600]\n",
    "# 结果记录\n",
    "all_results = []\n",
    "\n",
    "label_csv_files = sorted([f for f in os.listdir(label_folder_path) if f.endswith('.csv')])\n",
    "sample_csv_files = sorted([f for f in os.listdir(sample_folder_path) if f.endswith('.csv')])\n",
    "\n",
    "# 用于存储所有标签和样本的 DataFrame\n",
    "all_labels = []\n",
    "all_samples = []\n",
    "\n",
    "# 加载所有标签和样本文件\n",
    "for label_file, sample_file in zip(label_csv_files, sample_csv_files):\n",
    "    all_labels.append(pd.read_csv(os.path.join(label_folder_path, label_file)))\n",
    "    all_samples.append(pd.read_csv(os.path.join(sample_folder_path, sample_file)))\n",
    "\n",
    "# 合并所有标签和样本\n",
    "all_labels_df = pd.concat(all_labels, axis=0)\n",
    "all_samples_df = pd.concat(all_samples, axis=0)\n",
    "\n",
    "# 合并样本和标签数据\n",
    "merged_df = pd.merge(all_samples_df, all_labels_df, on='sample', how='left')\n",
    "\n",
    "# 调用 equal_kind 函数处理合并后的数据\n",
    "merged_df = equal_kind(merged_df)\n",
    "# 将特征列倒序\n",
    "first_col = merged_df.iloc[:, 0]\n",
    "last_col = merged_df.iloc[:, -1]\n",
    "middle_cols_reversed = merged_df.iloc[:, 1:-1].iloc[:, ::-1]\n",
    "merged_df_reordered = pd.concat([first_col, middle_cols_reversed, last_col], axis=1)\n",
    "\n",
    "# 进行循环\n",
    "for features_num in features_nums:\n",
    "    print(f\"Running with features_num = {features_num}\")\n",
    "    \n",
    "    # 准备数据集\n",
    "    X = merged_df_reordered.iloc[:, 1:features_num + 1].values  # 提取特征列\n",
    "    y = merged_df_reordered.iloc[:, -1].values  # 提取标签列\n",
    "    # 五折交叉验证\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_auroc = []\n",
    "    fold_F1 = []\n",
    "    fold_aurp = []\n",
    "\n",
    "    # 五折交叉验证\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        \n",
    "        # 获取当前折的训练集和验证集\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "\n",
    "        # 在当前折上进行训练和验证\n",
    "        acc, auroc, F1, aurp = LR_kftest(X_train, y_train, X_val, y_val, SEED)\n",
    "        \n",
    "        fold_accuracies.append(acc)\n",
    "        fold_auroc.append(auroc)\n",
    "        fold_F1.append(F1)\n",
    "        fold_aurp.append(aurp)\n",
    "\n",
    "    # 计算五折的平均值和方差\n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    acc_variance = np.var(fold_accuracies)\n",
    "    mean_auroc = np.mean(fold_auroc)\n",
    "    auroc_variance = np.var(fold_auroc)\n",
    "    mean_F1  = np.mean(fold_F1)\n",
    "    F1_variance = np.var(fold_F1)\n",
    "    mean_aurp = np.mean(fold_aurp)\n",
    "    aurp_variance = np.var(fold_aurp)\n",
    "\n",
    "    # 设置当前实验的结果\n",
    "    results = {\n",
    "        \"features_num\": features_num,\n",
    "        \"mean_accuracy\": mean_acc,\n",
    "        \"accuracy_variance\": acc_variance,\n",
    "        \"mean_auroc\": mean_auroc,\n",
    "        \"auroc_variance\": auroc_variance,\n",
    "        \"mean_F1\": mean_F1,\n",
    "        \"F1_variance\": F1_variance,\n",
    "        \"mean_aurp\": mean_aurp,\n",
    "        \"aurp_variance\": aurp_variance\n",
    "    }\n",
    "\n",
    "    # 将当前实验的结果添加到结果列表中\n",
    "    all_results.append(results)\n",
    "\n",
    "# 结果保存路径\n",
    "csv_file = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/ML_models/eight_sample_11features_test/5fold_features_ablation/LR_ablation_reveresd_results.csv'\n",
    "\n",
    "# 检查文件是否存在\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# 打开文件并追加结果\n",
    "with open(csv_file, mode='a', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=results.keys())\n",
    "    \n",
    "    # 如果文件不存在，写入标题\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # 写入每次实验的结果\n",
    "    for result in all_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(\"Experiment completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>cov_1</th>\n",
       "      <th>cov_2</th>\n",
       "      <th>cov_3</th>\n",
       "      <th>cov_4</th>\n",
       "      <th>cov_5</th>\n",
       "      <th>cov_6</th>\n",
       "      <th>cov_7</th>\n",
       "      <th>cov_8</th>\n",
       "      <th>cov_9</th>\n",
       "      <th>...</th>\n",
       "      <th>struc112</th>\n",
       "      <th>struc113</th>\n",
       "      <th>struc114</th>\n",
       "      <th>struc115</th>\n",
       "      <th>struc116</th>\n",
       "      <th>struc117</th>\n",
       "      <th>struc118</th>\n",
       "      <th>struc119</th>\n",
       "      <th>struc120</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>promoter__chr17___16439436____16443036_pos|598...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>promoter__chr6___26312414____26314414_neg|1800...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enhancer__chr9___35657203____35663203_neg|5183...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NR_145822_____1|4185|diff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enhancer__chr6___27662663____27671263_neg|678|...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>L1MEd__chr6___157012070____157012304_pos|136|diff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>ENST00000385230_____1|21|same</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>ENST00000408479_____1|71|same</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>NR_146117_____1|11610|same</td>\n",
       "      <td>0.365759</td>\n",
       "      <td>0.346304</td>\n",
       "      <td>0.175097</td>\n",
       "      <td>0.089494</td>\n",
       "      <td>0.085603</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>ENST00000385011_____1|26|same</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2676 rows × 642 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sample     cov_1     cov_2  \\\n",
       "0     promoter__chr17___16439436____16443036_pos|598...  0.000000  0.000000   \n",
       "1     promoter__chr6___26312414____26314414_neg|1800...  0.000000  0.000000   \n",
       "2     enhancer__chr9___35657203____35663203_neg|5183...  0.000000  0.000000   \n",
       "3                             NR_145822_____1|4185|diff  0.000000  0.000000   \n",
       "4     enhancer__chr6___27662663____27671263_neg|678|...  0.000000  0.000000   \n",
       "...                                                 ...       ...       ...   \n",
       "2671  L1MEd__chr6___157012070____157012304_pos|136|diff  0.000000  0.000000   \n",
       "2672                      ENST00000385230_____1|21|same  0.000000  0.000000   \n",
       "2673                      ENST00000408479_____1|71|same  0.000000  0.000000   \n",
       "2674                         NR_146117_____1|11610|same  0.365759  0.346304   \n",
       "2675                      ENST00000385011_____1|26|same  0.000000  0.000000   \n",
       "\n",
       "         cov_3     cov_4     cov_5     cov_6     cov_7     cov_8     cov_9  \\\n",
       "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2671  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2672  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2673  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2674  0.175097  0.089494  0.085603  0.046693  0.046693  0.046693  0.046693   \n",
       "2675  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      ...  struc112  struc113  struc114  struc115  struc116  struc117  \\\n",
       "0     ...         1         0         0         1         0         0   \n",
       "1     ...         1         0         0         1         0         0   \n",
       "2     ...         0         0         1         1         0         0   \n",
       "3     ...         1         0         0         1         0         0   \n",
       "4     ...         1         0         0         1         0         0   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "2671  ...         1         0         0         1         0         0   \n",
       "2672  ...         1         0         0         1         0         0   \n",
       "2673  ...         1         0         0         1         0         0   \n",
       "2674  ...         0         0         1         1         0         0   \n",
       "2675  ...         1         0         0         1         0         0   \n",
       "\n",
       "      struc118  struc119  struc120  label  \n",
       "0            1         0         0      0  \n",
       "1            1         0         0      1  \n",
       "2            1         0         0      0  \n",
       "3            1         0         0      1  \n",
       "4            1         0         0      1  \n",
       "...        ...       ...       ...    ...  \n",
       "2671         1         0         0      1  \n",
       "2672         1         0         0      0  \n",
       "2673         1         0         0      0  \n",
       "2674         1         0         0      0  \n",
       "2675         1         0         0      0  \n",
       "\n",
       "[2676 rows x 642 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/labels/'\n",
    "sample_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/samples/'\n",
    "# 定义不同的 features_num\n",
    "features_nums = [640, 600, 560, 520, 480, 440, 400, 360, 320, 280, 240, 200, 160, 120, 80, 40]\n",
    "# features_nums = [600]\n",
    "# 结果记录\n",
    "all_results = []\n",
    "\n",
    "label_csv_files = sorted([f for f in os.listdir(label_folder_path) if f.endswith('.csv')])\n",
    "sample_csv_files = sorted([f for f in os.listdir(sample_folder_path) if f.endswith('.csv')])\n",
    "\n",
    "# 用于存储所有标签和样本的 DataFrame\n",
    "all_labels = []\n",
    "all_samples = []\n",
    "\n",
    "# 加载所有标签和样本文件\n",
    "for label_file, sample_file in zip(label_csv_files, sample_csv_files):\n",
    "    all_labels.append(pd.read_csv(os.path.join(label_folder_path, label_file)))\n",
    "    all_samples.append(pd.read_csv(os.path.join(sample_folder_path, sample_file)))\n",
    "\n",
    "# 合并所有标签和样本\n",
    "all_labels_df = pd.concat(all_labels, axis=0)\n",
    "all_samples_df = pd.concat(all_samples, axis=0)\n",
    "\n",
    "# 合并样本和标签数据\n",
    "merged_df = pd.merge(all_samples_df, all_labels_df, on='sample', how='left')\n",
    "\n",
    "# 调用 equal_kind 函数处理合并后的数据\n",
    "merged_df = equal_kind(merged_df)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>struc120</th>\n",
       "      <th>struc119</th>\n",
       "      <th>struc118</th>\n",
       "      <th>struc117</th>\n",
       "      <th>struc116</th>\n",
       "      <th>struc115</th>\n",
       "      <th>struc114</th>\n",
       "      <th>struc113</th>\n",
       "      <th>struc112</th>\n",
       "      <th>...</th>\n",
       "      <th>cov_9</th>\n",
       "      <th>cov_8</th>\n",
       "      <th>cov_7</th>\n",
       "      <th>cov_6</th>\n",
       "      <th>cov_5</th>\n",
       "      <th>cov_4</th>\n",
       "      <th>cov_3</th>\n",
       "      <th>cov_2</th>\n",
       "      <th>cov_1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>promoter__chr17___16439436____16443036_pos|598...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>promoter__chr6___26312414____26314414_neg|1800...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enhancer__chr9___35657203____35663203_neg|5183...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NR_145822_____1|4185|diff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enhancer__chr6___27662663____27671263_neg|678|...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>L1MEd__chr6___157012070____157012304_pos|136|diff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>ENST00000385230_____1|21|same</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>ENST00000408479_____1|71|same</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>NR_146117_____1|11610|same</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.085603</td>\n",
       "      <td>0.089494</td>\n",
       "      <td>0.175097</td>\n",
       "      <td>0.346304</td>\n",
       "      <td>0.365759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>ENST00000385011_____1|26|same</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2676 rows × 642 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sample  struc120  struc119  \\\n",
       "0     promoter__chr17___16439436____16443036_pos|598...         0         0   \n",
       "1     promoter__chr6___26312414____26314414_neg|1800...         0         0   \n",
       "2     enhancer__chr9___35657203____35663203_neg|5183...         0         0   \n",
       "3                             NR_145822_____1|4185|diff         0         0   \n",
       "4     enhancer__chr6___27662663____27671263_neg|678|...         0         0   \n",
       "...                                                 ...       ...       ...   \n",
       "2671  L1MEd__chr6___157012070____157012304_pos|136|diff         0         0   \n",
       "2672                      ENST00000385230_____1|21|same         0         0   \n",
       "2673                      ENST00000408479_____1|71|same         0         0   \n",
       "2674                         NR_146117_____1|11610|same         0         0   \n",
       "2675                      ENST00000385011_____1|26|same         0         0   \n",
       "\n",
       "      struc118  struc117  struc116  struc115  struc114  struc113  struc112  \\\n",
       "0            1         0         0         1         0         0         1   \n",
       "1            1         0         0         1         0         0         1   \n",
       "2            1         0         0         1         1         0         0   \n",
       "3            1         0         0         1         0         0         1   \n",
       "4            1         0         0         1         0         0         1   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2671         1         0         0         1         0         0         1   \n",
       "2672         1         0         0         1         0         0         1   \n",
       "2673         1         0         0         1         0         0         1   \n",
       "2674         1         0         0         1         1         0         0   \n",
       "2675         1         0         0         1         0         0         1   \n",
       "\n",
       "      ...     cov_9     cov_8     cov_7     cov_6     cov_5     cov_4  \\\n",
       "0     ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4     ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "2671  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2672  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2673  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2674  ...  0.046693  0.046693  0.046693  0.046693  0.085603  0.089494   \n",
       "2675  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         cov_3     cov_2     cov_1  label  \n",
       "0     0.000000  0.000000  0.000000      0  \n",
       "1     0.000000  0.000000  0.000000      1  \n",
       "2     0.000000  0.000000  0.000000      0  \n",
       "3     0.000000  0.000000  0.000000      1  \n",
       "4     0.000000  0.000000  0.000000      1  \n",
       "...        ...       ...       ...    ...  \n",
       "2671  0.000000  0.000000  0.000000      1  \n",
       "2672  0.000000  0.000000  0.000000      0  \n",
       "2673  0.000000  0.000000  0.000000      0  \n",
       "2674  0.175097  0.346304  0.365759      0  \n",
       "2675  0.000000  0.000000  0.000000      0  \n",
       "\n",
       "[2676 rows x 642 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "merged_df_reordered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
