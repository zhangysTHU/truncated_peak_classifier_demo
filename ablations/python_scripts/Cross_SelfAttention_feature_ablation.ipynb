{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, average_precision_score\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(net, test_dataloader):\n",
    "    net.eval()  # 设置模型为评估模式\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in test_dataloader:\n",
    "            inputs = sample[0]\n",
    "            labels = sample[1]\n",
    "            labels = labels.unsqueeze(1)  # 确保标签的形状为 (batch_size, 1)\n",
    "            labels = labels.float()\n",
    "\n",
    "            # 获取模型输出，经过sigmoid后得到概率\n",
    "            probabilities = torch.sigmoid(net(inputs))  # 二分类任务，输出一个概率值\n",
    "\n",
    "            # 由于输出是一个概率，直接使用它来作为正类的概率\n",
    "            y_scores.extend(probabilities.numpy())\n",
    "            y_true.extend(labels.numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "\n",
    "    # 计算F1 Score和AUC值\n",
    "    y_pred = np.round(y_scores)  # 将概率转换为0或1\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    aupr = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    # 返回AUC, F1, AUPR\n",
    "    return auc, f1, aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ColumnAttention, self).__init__()\n",
    "        # 自注意力的权重\n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 输入 x 维度: (batch_size, 40, 13)\n",
    "        Q = self.query(x)  # (batch_size, 40, hidden_dim)\n",
    "        K = self.key(x)    # (batch_size, 40, hidden_dim)\n",
    "        V = self.value(x)  # (batch_size, 40, hidden_dim)\n",
    "        \n",
    "        # 计算注意力分数: Q * K^T\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, 40, 40)\n",
    "        attention_scores = attention_scores / (K.size(-1) ** 0.5)  # 缩放\n",
    "        \n",
    "        # 通过softmax获得注意力权重\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, 40, 40)\n",
    "        \n",
    "        # 应用注意力权重到值上\n",
    "        attended_values = torch.matmul(attention_weights, V)  # (batch_size, 40, hidden_dim)\n",
    "        output = self.fc(attended_values) # (batch_size, 40, 13)\n",
    "        return output\n",
    "    \n",
    "class RowAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RowAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)  # (batch_size, 13, hidden_dim)\n",
    "        K = self.key(x)    # (batch_size, 13, hidden_dim)\n",
    "        V = self.value(x)  # (batch_size, 13, hidden_dim)\n",
    "        \n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, 13, 13)\n",
    "        attention_scores = attention_scores / (K.size(-1) ** 0.5)  # Scaling\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, 13, 13)\n",
    "        \n",
    "        attended_values = torch.matmul(attention_weights, V)  # (batch_size, 13, hidden_dim)\n",
    "        output = self.fc(attended_values)  # (batch_size, 13, 40)\n",
    "        return output\n",
    "\n",
    "class Feed_Forward(nn.Module):\n",
    "    def __init__(self, input_dim, d_ff, dropout_rate=0.1):\n",
    "        super(Feed_Forward, self).__init__()\n",
    "        # 第一个全连接层，输入维度为 input_dim，输出维度为 d_ff\n",
    "        self.fc1 = nn.Linear(input_dim, d_ff)\n",
    "        # 第二个全连接层，输入维度为 d_ff，输出维度为 input_dim\n",
    "        self.fc2 = nn.Linear(d_ff, input_dim)\n",
    "        # Dropout层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # 激活函数：ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过第一个全连接层\n",
    "        x1 = self.fc1(x)\n",
    "        # 激活函数\n",
    "        x1 = self.relu(x1)\n",
    "        # Dropout\n",
    "        x1 = self.dropout(x1)\n",
    "        # 通过第二个全连接层\n",
    "        x2 = self.fc2(x1)\n",
    "        # 残差连接：输入与输出相加\n",
    "        out = x + x2  # x 是输入，x2 是第二个全连接层的输出\n",
    "        return out\n",
    "\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim=13, seq_len=40, hidden_dim=64, output_dim=1, d_ff= 32, dropout_rate = 0.15):\n",
    "        super(CrossAttentionModel, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.column_attention = ColumnAttention(input_dim, hidden_dim)\n",
    "        self.row_attention = RowAttention(input_dim = 40, hidden_dim = 64)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "        self.ffn1 = Feed_Forward(input_dim, d_ff, dropout_rate)\n",
    "        self.ffn2 = Feed_Forward(input_dim, d_ff, dropout_rate)\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(input_dim * seq_len, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.droupout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入 x 维度: (batch_size, 40, 13)\n",
    "        x = x.view(-1, self.seq_len, self.input_dim)  # 调整输入形状\n",
    "        x = self.column_attention(x)\n",
    "        x = self.layer_norm1(x)  # Layer Norm after Column Attention\n",
    "        x = self.ffn1(x)  # Feed-Forward Neural Network\n",
    "        # return x\n",
    "        # Step 2: Row Attention\n",
    "        x = x.transpose(1, 2)  # Change shape to (batch_size, 13, 40) for row attention\n",
    "        # return x\n",
    "        x = self.row_attention(x)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        # return x\n",
    "        x = self.layer_norm2(x)  # Layer Norm after Row Attention\n",
    "        x = self.ffn2(x)  # Feed-Forward Neural Network\n",
    "        \n",
    "        # 将输出展平\n",
    "        attention_output = x.view(-1, self.seq_len * self.input_dim)\n",
    "        \n",
    "        # 全连接层\n",
    "        x = F.relu(self.fc1(attention_output))  # (batch_size, 128)\n",
    "        x = self.droupout(x) # (batch_size, 128)\n",
    "        x = self.fc2(x)  # (batch_size, output_dim = 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # 对column_attention的linear层进行初始化\n",
    "        init.kaiming_normal_(self.column_attention.query.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.column_attention.key.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.column_attention.value.weight, nonlinearity='relu')\n",
    "        \n",
    "        # 对fc1和fc2进行初始化\n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        \n",
    "        # 初始化偏置项为0\n",
    "        if self.column_attention.query.bias is not None:\n",
    "            init.constant_(self.column_attention.query.bias, 0)\n",
    "            init.constant_(self.column_attention.key.bias, 0)\n",
    "            init.constant_(self.column_attention.value.bias, 0)\n",
    "        \n",
    "        if self.fc1.bias is not None:\n",
    "            init.constant_(self.fc1.bias, 0)\n",
    "        if self.fc2.bias is not None:\n",
    "            init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "class MultiColumnAttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim=16, seq_len=40, hidden_dim=64, output_dim=1, d_ff= 32, dropout_rate = 0.15):\n",
    "        super(CrossAttentionModel, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.column_attention = ColumnAttention(input_dim, hidden_dim)\n",
    "        self.row_attention = RowAttention(input_dim = 40, hidden_dim = 64)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "        self.ffn1 = Feed_Forward(input_dim, d_ff, dropout_rate)\n",
    "        self.ffn2 = Feed_Forward(input_dim, d_ff, dropout_rate)\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(input_dim * seq_len, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.droupout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入 x 维度: (batch_size, 40, 16)\n",
    "        x = x.view(-1, self.seq_len, self.input_dim)  # 调整输入形状\n",
    "\n",
    "        x = self.column_attention(x)\n",
    "        x = self.layer_norm1(x)  # Layer Norm after Column Attention\n",
    "        x = self.ffn1(x)  # Feed-Forward Neural Network,(batch_size, 16, 40)\n",
    "\n",
    "        x = self.column_attention(x)\n",
    "        x = self.layer_norm2(x)  # Layer Norm after Row Attention\n",
    "        x = self.ffn2(x)  # Feed-Forward Neural Network,(batch_size, 16, 40)\n",
    "        \n",
    "        # 将输出展平\n",
    "        attention_output = x.view(-1, self.seq_len * self.input_dim)\n",
    "        \n",
    "        # 全连接层\n",
    "        x = F.relu(self.fc1(attention_output))  # (batch_size, 128)\n",
    "        x = self.droupout(x) # (batch_size, 128)\n",
    "        x = self.fc2(x)  # (batch_size, output_dim = 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # 对column_attention的linear层进行初始化\n",
    "        init.kaiming_normal_(self.column_attention.query.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.column_attention.key.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.column_attention.value.weight, nonlinearity='relu')\n",
    "        \n",
    "        # 对fc1和fc2进行初始化\n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        \n",
    "        # 初始化偏置项为0\n",
    "        if self.column_attention.query.bias is not None:\n",
    "            init.constant_(self.column_attention.query.bias, 0)\n",
    "            init.constant_(self.column_attention.key.bias, 0)\n",
    "            init.constant_(self.column_attention.value.bias, 0)\n",
    "        \n",
    "        if self.fc1.bias is not None:\n",
    "            init.constant_(self.fc1.bias, 0)\n",
    "        if self.fc2.bias is not None:\n",
    "            init.constant_(self.fc2.bias, 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"\n",
    "    在n个变量上累加\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n       # 创建一个长度为 n 的列表，初始化所有元素为0.0。\n",
    "\n",
    "    def add(self, *args):           # 累加\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):                # 重置累加器的状态，将所有元素重置为0.0\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):     # 获取所有数据\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"\n",
    "    计算正确的数量\n",
    "    :param y_hat:\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "    #     y_hat = y_hat.argmax(axis=1)            # 在每行中找到最大值的索引，以确定每个样本的预测类别\n",
    "    # cmp = y_hat.type(y.dtype) == y\n",
    "    # return float(cmp.type(y.dtype).sum())\n",
    "    y_hat = (y_hat >= 0.5).float()  # 将概率转化为 0 或 1\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    \"\"\"\n",
    "    计算指定数据集的精度\n",
    "    :param net:\n",
    "    :param data_iter:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()                  # 通常会关闭一些在训练时启用的行为\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for sample in data_iter:\n",
    "            X = sample[0]\n",
    "            y = sample[1]\n",
    "            y_hat = net(X)\n",
    "            y = y.unsqueeze(1)\n",
    "            y = y.float()\n",
    "            metric.add(accuracy(y_hat, y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    \"\"\"\n",
    "    训练模型一轮\n",
    "    :param net:是要训练的神经网络模型\n",
    "    :param train_iter:是训练数据的数据迭代器，用于遍历训练数据集\n",
    "    :param loss:是用于计算损失的损失函数\n",
    "    :param updater:是用于更新模型参数的优化器\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(net, torch.nn.Module):  # 用于检查一个对象是否属于指定的类（或类的子类）或数据类型。\n",
    "        net.train()\n",
    "\n",
    "    # 训练损失总和， 训练准确总和， 样本数\n",
    "    metric = Accumulator(3)\n",
    "\n",
    "    for sample in train_iter:\n",
    "        X = sample[0]\n",
    "        y = sample[1]\n",
    "        # X = X.view(X.shape[0], 1, -1)\n",
    "        y_hat = net(X)\n",
    "        y = y.unsqueeze(1)\n",
    "        y = y.float()\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):  # 用于检查一个对象是否属于指定的类（或类的子类）或数据类型。\n",
    "            # 使用pytorch内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()  # 方法用于计算损失的平均值\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制（自定义）的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape())\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "# ch6版本增加了valset和testset参数，在训练结束后对测试机进行评估\n",
    "def train_ch6(net, train_iter, val_iter, test_iter, loss, num_epochs, updater, scheduler = None, save_best=True, checkpoint_dir='./selfattention_checkpoints'):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    :param net: 模型\n",
    "    :param train_iter: 训练数据的迭代器\n",
    "    :param test_iter: 验证数据的迭代器，用于早停\n",
    "    :param test_iter: 测试数据的迭代器\n",
    "    :param loss: 损失函数\n",
    "    :param num_epochs: 训练的轮数\n",
    "    :param updater: 参数更新器/优化器\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    \n",
    "    best_acc = 0.0  # 跟踪最高的测试精度\n",
    "    best_epoch = 0  # 保存最佳模型的epoch\n",
    "    best_auroc = 0 \n",
    "    best_F1 = 0\n",
    "    best_auc =0.0\n",
    "    best_aurp = 0 #最佳的auroc，F1，aurp都跟随最佳accuracy\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练模型一轮，并返回训练损失和训练精度\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        # 计算测试集上的精度\n",
    "        test_acc = evaluate_accuracy(net, val_iter)\n",
    "        # 计算验证集上的AUROC\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        net.eval()  # 设置模型为评估模式\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_iter:\n",
    "                outputs = net(X_batch)\n",
    "                all_labels.append(y_batch.detach().numpy())\n",
    "                all_preds.append(outputs.detach().numpy()) # 直接使用原始输出作为预测值，sklearn的roc_auc_score不需要归一化为的概率为输入\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        test_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "        if  test_auc > best_auc:\n",
    "            best_auc = test_auc\n",
    "            best_epoch = epoch\n",
    "            # 获取文件夹中已有的模型文件\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'best_model_epoch_{epoch + 1}_auc_{best_auc:.3f}.pth')\n",
    "            # 如果存在上一个保存的模型，删除它\n",
    "            if 'previous_checkpoint_path' in locals() and os.path.exists(previous_checkpoint_path):\n",
    "                os.remove(previous_checkpoint_path)\n",
    "            torch.save(net.state_dict(), checkpoint_path)       \n",
    "            # 更新记录当前模型的路径\n",
    "            previous_checkpoint_path = checkpoint_path\n",
    "            \n",
    "    # 获取本次训练得到的最佳模型\n",
    "    best_model_path = os.path.join(checkpoint_dir, f'best_model_epoch_{best_epoch + 1}_auc_{best_auc:.3f}.pth')\n",
    "    saved_model = net\n",
    "    saved_model.load_state_dict(torch.load(best_model_path))\n",
    "    saved_model.eval()  # 切换为评估模式\n",
    "    best_auroc, best_F1, best_aurp = model_evaluate(saved_model, test_iter)\n",
    "    return best_acc, best_auroc, best_F1, best_aurp\n",
    "\n",
    "\n",
    "# ch7版本去除了test set，专门用于不早停的五折交叉验证\n",
    "def train_ch7(net, train_iter, val_iter, loss, num_epochs, updater, scheduler = None,  checkpoint_dir='./selfattention_checkpoints'):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    :param net: 模型\n",
    "    :param train_iter: 训练数据的迭代器\n",
    "    :param val_iter: 验证数据的迭代器\n",
    "    :param loss: 损失函数\n",
    "    :param num_epochs: 训练的轮数\n",
    "    :param updater: 参数更新器/优化器\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    \n",
    "    best_acc = 0.0  # 跟踪最高的测试精度\n",
    "    # best_epoch = 0  # 保存最佳模型的epoch\n",
    "    best_auroc = 0 \n",
    "    best_F1 = 0\n",
    "    best_auc =0.0\n",
    "    best_aurp = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练模型一轮，并返回训练损失和训练精度\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        # 计算测试集上的精度\n",
    "        test_acc = evaluate_accuracy(net, val_iter)\n",
    "        # 计算验证集上的AUROC\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        net.eval()  # 设置模型为评估模式\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_iter:\n",
    "                outputs = net(X_batch)\n",
    "                all_labels.append(y_batch.detach().numpy())\n",
    "                all_preds.append(outputs.detach().numpy()) # 直接使用原始输出作为预测值，sklearn的roc_auc_score不需要归一化为的概率为输入\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        test_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "        if  epoch == num_epochs - 1:\n",
    "            best_auc = test_auc\n",
    "            # best_epoch = epoch\n",
    "            best_acc = test_acc\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'best_model_epoch_{epoch + 1}_auc_{best_auc:.3f}.pth')\n",
    "            torch.save(net.state_dict(), checkpoint_path)       \n",
    "            \n",
    "    # 获取本次训练得到的最佳模型\n",
    "    saved_model = net\n",
    "    saved_model.load_state_dict(torch.load(checkpoint_path))\n",
    "    saved_model.eval()  # 切换为评估模式\n",
    "    best_auroc, best_F1, best_aurp = model_evaluate(saved_model, val_iter)\n",
    "    return best_acc, best_auroc, best_F1, best_aurp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset_pos_encoded(Dataset):\n",
    "    def __init__(self, features, labels, input_dim=16, seq_len=40):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.input_dim = input_dim  # 动态传入的特征维度\n",
    "        self.seq_len = seq_len      # 序列长度\n",
    "        self.position_encoding = self.create_position_encoding(seq_len, input_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 将每个样本的特征转换为 (seq_len, input_dim) 的二维张量\n",
    "        sample = self.features[idx].reshape(self.input_dim, self.seq_len).astype(np.float32).T  # 转置为 (input_dim, seq_len)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        position_encoding = self.position_encoding.numpy()  # 获取位置编码，shape: (seq_len, input_dim)\n",
    "        \n",
    "        # 逐元素相加样本和位置编码\n",
    "        sample_with_pos = sample + position_encoding  # shape: (seq_len, input_dim)\n",
    "\n",
    "        return torch.tensor(sample_with_pos), label\n",
    "\n",
    "    def create_position_encoding(self, seq_len, input_dim):\n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, input_dim, 2) * -(np.log(10000.0) / input_dim))\n",
    "\n",
    "        pos_enc = np.zeros((seq_len, input_dim))\n",
    "        pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "\n",
    "        if input_dim % 2 != 0:\n",
    "            pos_enc[:, 1::2] = np.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "\n",
    "        return torch.tensor(pos_enc, dtype=torch.float32)  # 将生成的 numpy 数组转换为 torch tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels, input_dim=16, seq_len=40):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.input_dim = input_dim  # 动态传入的特征维度\n",
    "        self.seq_len = seq_len      # 序列长度\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 将每个样本的特征转换为 (seq_len, input_dim) 的二维张量\n",
    "        sample = self.features[idx].reshape(self.input_dim, self.seq_len).astype(np.float32).T  # 转置为 (input_dim, seq_len)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return torch.tensor(sample), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_kind(data_df):\n",
    "    df_y0 = data_df[data_df['label'] == 0]\n",
    "    df_y1 = data_df[data_df['label'] == 1]\n",
    "\n",
    "    # 确定两个子集中数量较少的那个\n",
    "    min_count = min(len(df_y0), len(df_y1))\n",
    "\n",
    "    # 从两个子集中随机选择等量的样本\n",
    "    df_y0_balanced = df_y0.sample(n=min_count, random_state=42) if len(df_y0) > min_count else df_y0\n",
    "    df_y1_balanced = df_y1.sample(n=min_count, random_state=42) if len(df_y1) > min_count else df_y1\n",
    "\n",
    "    # 合并这两个平衡后的子集\n",
    "    balanced_df = pd.concat([df_y0_balanced, df_y1_balanced])\n",
    "    # 打乱合并后的数据集的顺序\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并样本并划分训练集和测试集的函数\n",
    "def prepare_dataset(label_folder_path, sample_folder_path, test_sample_name=None, random_split=True, test_size=0.1, features_num = 640):\n",
    "    # 获取所有 CSV 文件的文件名，并按文件名排序\n",
    "    label_csv_files = sorted([f for f in os.listdir(label_folder_path) if f.endswith('.csv')])\n",
    "    sample_csv_files = sorted([f for f in os.listdir(sample_folder_path) if f.endswith('.csv')])\n",
    "\n",
    "    # 创建空的列表，用于存储所有的 DataFrame（除了测试集的文件）\n",
    "    train_labels = []\n",
    "    train_samples = []\n",
    "\n",
    "    # 创建空的列表，用于存储测试集的 DataFrame\n",
    "    test_label = None\n",
    "    test_sample = None\n",
    "\n",
    "    # 判断是否按指定的测试集样本名称进行划分\n",
    "    if test_sample_name:\n",
    "        for label_file, sample_file in zip(label_csv_files, sample_csv_files):\n",
    "            if test_sample_name in label_file:  # 判断是否为指定的测试集\n",
    "                test_label = pd.read_csv(os.path.join(label_folder_path, label_file))\n",
    "                test_sample = pd.read_csv(os.path.join(sample_folder_path, sample_file))\n",
    "            else:\n",
    "                train_labels.append(pd.read_csv(os.path.join(label_folder_path, label_file)))\n",
    "                train_samples.append(pd.read_csv(os.path.join(sample_folder_path, sample_file)))\n",
    "                \n",
    "    elif random_split:  # 如果是随机划分测试集\n",
    "        # 随机选取 10% 数据作为测试集\n",
    "        all_labels = [pd.read_csv(os.path.join(label_folder_path, file)) for file in label_csv_files]\n",
    "        all_samples = [pd.read_csv(os.path.join(sample_folder_path, file)) for file in sample_csv_files]\n",
    "\n",
    "        # 合并所有的样本和标签\n",
    "        all_labels_df = pd.concat(all_labels, axis=0)\n",
    "        all_samples_df = pd.concat(all_samples, axis=0)\n",
    "\n",
    "        # 随机划分训练集和测试集\n",
    "        train_samples_df, test_samples_df, train_labels_df, test_labels_df = train_test_split(\n",
    "            all_samples_df, all_labels_df, test_size=test_size, random_state=42\n",
    "        )\n",
    "\n",
    "        # 合并训练集特征和标签\n",
    "        merged_train_df = pd.merge(train_samples_df, train_labels_df, on='sample', how='left')\n",
    "        merged_train_df = equal_kind(merged_train_df)\n",
    "\n",
    "        # 合并测试集特征和标签\n",
    "        merged_test_df = pd.merge(test_samples_df, test_labels_df, on='sample', how='left')\n",
    "        merged_test_df = equal_kind(merged_test_df)\n",
    "\n",
    "        # 提取特征和标签\n",
    "        train_features = merged_train_df.iloc[:, 1: features_num + 1].values  # 第一列为 'sample'，最后一列为 'label'\n",
    "        train_labels = merged_train_df.iloc[:, -1].values  # 标签列\n",
    "\n",
    "        test_features = merged_test_df.iloc[:, 1: features_num + 1].values\n",
    "        test_labels = merged_test_df.iloc[:, -1].values\n",
    "\n",
    "        return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "    else:\n",
    "        # 按照默认方式,即指定一个样本作为测试集\n",
    "        for label_file, sample_file in zip(label_csv_files, sample_csv_files):\n",
    "            train_labels.append(pd.read_csv(os.path.join(label_folder_path, label_file)))\n",
    "            train_samples.append(pd.read_csv(os.path.join(sample_folder_path, sample_file)))\n",
    "\n",
    "        # 将训练集的 DataFrame 合并\n",
    "        train_y_df = pd.concat(train_labels, axis=0)\n",
    "        train_X_df = pd.concat(train_samples, axis=0)\n",
    "\n",
    "        # 合并训练集的特征和标签\n",
    "        merged_train_df = pd.merge(train_X_df, train_y_df, on='sample', how='left')\n",
    "        merged_train_df = equal_kind(merged_train_df)  # 继续调用你的equal_kind函数进行处理\n",
    "\n",
    "        # 提取训练集的特征和标签\n",
    "        train_features = merged_train_df.iloc[:, 1:features_num + 1].values  # 第一列为 'sample'，最后一列为 'label'\n",
    "        train_labels = merged_train_df.iloc[:, features_num + 1].values  # 标签列\n",
    "\n",
    "        return train_features, train_labels, None, None\n",
    "    \n",
    "# b不进行测试集划分，直接合并所有文件\n",
    "def prepare_full_dataset(label_folder_path, sample_folder_path, features_num):\n",
    "    \"\"\"\n",
    "    直接将所有样本数据和标签合并为一个 dataset，并返回特征和标签。\n",
    "    \n",
    "    :param label_folder_path: 标签文件夹路径\n",
    "    :param sample_folder_path: 样本文件夹路径\n",
    "    :param features_num: 用于训练的特征数量\n",
    "    :return: 特征和标签的 numpy 数组\n",
    "    \"\"\"\n",
    "    # 获取所有 CSV 文件的文件名，并按文件名排序\n",
    "    label_csv_files = sorted([f for f in os.listdir(label_folder_path) if f.endswith('.csv')])\n",
    "    sample_csv_files = sorted([f for f in os.listdir(sample_folder_path) if f.endswith('.csv')])\n",
    "\n",
    "    # 用于存储所有标签和样本的 DataFrame\n",
    "    all_labels = []\n",
    "    all_samples = []\n",
    "\n",
    "    # 加载所有标签和样本文件\n",
    "    for label_file, sample_file in zip(label_csv_files, sample_csv_files):\n",
    "        all_labels.append(pd.read_csv(os.path.join(label_folder_path, label_file)))\n",
    "        all_samples.append(pd.read_csv(os.path.join(sample_folder_path, sample_file)))\n",
    "\n",
    "    # 合并所有标签和样本\n",
    "    all_labels_df = pd.concat(all_labels, axis=0)\n",
    "    all_samples_df = pd.concat(all_samples, axis=0)\n",
    "\n",
    "    # 合并样本和标签数据\n",
    "    merged_df = pd.merge(all_samples_df, all_labels_df, on='sample', how='left')\n",
    "    \n",
    "    # 调用 equal_kind 函数处理合并后的数据\n",
    "    merged_df = equal_kind(merged_df)\n",
    "\n",
    "    # 提取特征和标签\n",
    "    features = merged_df.iloc[:, 1:features_num + 1].values  # 提取特征列\n",
    "    labels = merged_df.iloc[:, -1].values  # 提取标签列\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with features_num = 640\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The column label 'sample' is not unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_175031/1209813128.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeatures_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_nums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running with features_num = {features_num}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 准备数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_full_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 初始化模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossAttentionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_num\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175031/2247245691.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(label_folder_path, sample_folder_path, features_num)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mall_labels_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mall_samples_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# 合并样本和标签数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# 调用 equal_kind 函数处理合并后的数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mequal_kind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/BioII/lulab_b/huangkeyun/miniconda/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/BioII/lulab_b/huangkeyun/miniconda/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/BioII/lulab_b/huangkeyun/miniconda/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/BioII/lulab_b/huangkeyun/miniconda/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m                 \u001b[0mmulti_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m             \u001b[0mlabel_axis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"column\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1925\u001b[0m                 \u001b[0;34mf\"The {label_axis_name} label '{key}' is not unique.{multi_message}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             )\n\u001b[1;32m   1927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The column label 'sample' is not unique."
     ]
    }
   ],
   "source": [
    "label_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/SelfAttentionSamples/labels/'\n",
    "sample_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/SelfAttentionSamples/samples/'\n",
    "# 定义不同的 features_num\n",
    "features_nums = [640, 600, 560, 520, 480, 440, 400, 360, 320, 280, 240, 200, 160, 120, 80, 40]\n",
    "# features_nums = [600]\n",
    "# 结果记录\n",
    "all_results = []\n",
    "\n",
    "# 进行循环\n",
    "for features_num in features_nums:\n",
    "    print(f\"Running with features_num = {features_num}\")\n",
    "    \n",
    "    # 准备数据集\n",
    "    main_features, main_labels = prepare_full_dataset(label_folder_path, sample_folder_path, features_num=features_num)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = CrossAttentionModel(input_dim=features_num // 40, seq_len=40, hidden_dim=64, dropout_rate=0.15)\n",
    "    \n",
    "    # 训练参数\n",
    "    num_epochs = 60\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "\n",
    "    # 五折交叉验证\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_auroc = []\n",
    "    fold_F1 = []\n",
    "    fold_aurp = []\n",
    "\n",
    "    # 五折交叉验证\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(main_features)):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        \n",
    "        # 获取当前折的训练集和验证集\n",
    "        X_train, X_val = main_features[train_index], main_features[val_index]\n",
    "        y_train, y_val = main_labels[train_index], main_labels[val_index]\n",
    "\n",
    "        # 创建训练和验证数据集\n",
    "        train_dataset = CustomDataset_pos_encoded(X_train, y_train, input_dim=features_num // 40, seq_len=40)\n",
    "        val_dataset = CustomDataset_pos_encoded(X_val, y_val, input_dim=features_num // 40, seq_len=40)\n",
    "\n",
    "        # 创建训练和验证数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 初始化模型并优化器\n",
    "        model._initialize_weights()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # 在当前折上进行训练和验证\n",
    "        acc, auroc, F1, aurp = train_ch7(model, train_iter=train_loader, val_iter=val_loader, loss=loss_fn, num_epochs=num_epochs, updater=optimizer, checkpoint_dir=f'/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/ML_models/eight_sample_11features_test/DL_saved/single_column_11f_test0.1_findauc/fold{fold+1}/')\n",
    "        \n",
    "        fold_accuracies.append(acc)\n",
    "        fold_auroc.append(auroc)\n",
    "        fold_F1.append(F1)\n",
    "        fold_aurp.append(aurp)\n",
    "\n",
    "    # 计算五折的平均值和方差\n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    acc_variance = np.var(fold_accuracies)\n",
    "    mean_auroc = np.mean(fold_auroc)\n",
    "    auroc_variance = np.var(fold_auroc)\n",
    "    mean_F1  = np.mean(fold_F1)\n",
    "    F1_variance = np.var(fold_F1)\n",
    "    mean_aurp = np.mean(fold_aurp)\n",
    "    aurp_variance = np.var(fold_aurp)\n",
    "\n",
    "    # 设置当前实验的结果\n",
    "    results = {\n",
    "        \"features_num\": features_num,\n",
    "        \"mean_accuracy\": mean_acc,\n",
    "        \"accuracy_variance\": acc_variance,\n",
    "        \"mean_auroc\": mean_auroc,\n",
    "        \"auroc_variance\": auroc_variance,\n",
    "        \"mean_F1\": mean_F1,\n",
    "        \"F1_variance\": F1_variance,\n",
    "        \"mean_aurp\": mean_aurp,\n",
    "        \"aurp_variance\": aurp_variance\n",
    "    }\n",
    "\n",
    "    # 将当前实验的结果添加到结果列表中\n",
    "    all_results.append(results)\n",
    "\n",
    "# 结果保存路径\n",
    "csv_file = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/ML_models/eight_sample_11features_test/5fold_features_ablation/cross_ablation_results.csv'\n",
    "\n",
    "# 检查文件是否存在\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# 打开文件并追加结果\n",
    "with open(csv_file, mode='a', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=results.keys())\n",
    "    \n",
    "    # 如果文件不存在，写入标题\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # 写入每次实验的结果\n",
    "    for result in all_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(\"Experiment completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with features_num = 640\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The column label 'sample' is not unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_175031/1580958803.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeatures_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_nums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running with features_num = {features_num}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 准备数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_full_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# 初始化模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossAttentionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_num\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175031/2247245691.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(label_folder_path, sample_folder_path, features_num)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mall_labels_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mall_samples_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# 合并样本和标签数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# 调用 equal_kind 函数处理合并后的数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mequal_kind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/BioII/lulab_b/huangkeyun/miniconda/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/BioII/lulab_b/huangkeyun/miniconda/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/BioII/lulab_b/huangkeyun/miniconda/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/BioII/lulab_b/huangkeyun/miniconda/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m                 \u001b[0mmulti_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m             \u001b[0mlabel_axis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"column\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1925\u001b[0m                 \u001b[0;34mf\"The {label_axis_name} label '{key}' is not unique.{multi_message}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             )\n\u001b[1;32m   1927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The column label 'sample' is not unique."
     ]
    }
   ],
   "source": [
    "# 进行一次不带位置编码的学习\n",
    "label_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/SelfAttentionSamples/labels/'\n",
    "sample_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/SelfAttentionSamples/samples/'\n",
    "# 定义不同的 features_num\n",
    "features_nums = [640, 600, 560, 520, 480, 440, 400, 360, 320, 280, 240, 200, 160, 120, 80, 40]\n",
    "# features_nums = [600]\n",
    "# 结果记录\n",
    "all_results = []\n",
    "\n",
    "# 进行循环\n",
    "for features_num in features_nums:\n",
    "    print(f\"Running with features_num = {features_num}\")\n",
    "    \n",
    "    # 准备数据集\n",
    "    main_features, main_labels = prepare_full_dataset(label_folder_path, sample_folder_path, features_num=features_num)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = CrossAttentionModel(input_dim=features_num // 40, seq_len=40, hidden_dim=64, dropout_rate=0.15)\n",
    "    \n",
    "    # 训练参数\n",
    "    num_epochs = 60\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "\n",
    "    # 五折交叉验证\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_auroc = []\n",
    "    fold_F1 = []\n",
    "    fold_aurp = []\n",
    "\n",
    "    # 五折交叉验证\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(main_features)):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        \n",
    "        # 获取当前折的训练集和验证集\n",
    "        X_train, X_val = main_features[train_index], main_features[val_index]\n",
    "        y_train, y_val = main_labels[train_index], main_labels[val_index]\n",
    "\n",
    "        # 创建训练和验证数据集\n",
    "        train_dataset = CustomDataset(X_train, y_train, input_dim=features_num // 40, seq_len=40)\n",
    "        val_dataset = CustomDataset(X_val, y_val, input_dim=features_num // 40, seq_len=40)\n",
    "\n",
    "        # 创建训练和验证数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 初始化模型并优化器\n",
    "        model._initialize_weights()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # 在当前折上进行训练和验证\n",
    "        acc, auroc, F1, aurp = train_ch7(model, train_iter=train_loader, val_iter=val_loader, loss=loss_fn, num_epochs=num_epochs, updater=optimizer, checkpoint_dir=f'/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/ML_models/eight_sample_11features_test/DL_saved/single_column_11f_test0.1_findauc/fold{fold+1}/')\n",
    "        \n",
    "        fold_accuracies.append(acc)\n",
    "        fold_auroc.append(auroc)\n",
    "        fold_F1.append(F1)\n",
    "        fold_aurp.append(aurp)\n",
    "\n",
    "    # 计算五折的平均值和方差\n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    acc_variance = np.var(fold_accuracies)\n",
    "    mean_auroc = np.mean(fold_auroc)\n",
    "    auroc_variance = np.var(fold_auroc)\n",
    "    mean_F1  = np.mean(fold_F1)\n",
    "    F1_variance = np.var(fold_F1)\n",
    "    mean_aurp = np.mean(fold_aurp)\n",
    "    aurp_variance = np.var(fold_aurp)\n",
    "\n",
    "    # 设置当前实验的结果\n",
    "    results = {\n",
    "        \"features_num\": features_num,\n",
    "        \"mean_accuracy\": mean_acc,\n",
    "        \"accuracy_variance\": acc_variance,\n",
    "        \"mean_auroc\": mean_auroc,\n",
    "        \"auroc_variance\": auroc_variance,\n",
    "        \"mean_F1\": mean_F1,\n",
    "        \"F1_variance\": F1_variance,\n",
    "        \"mean_aurp\": mean_aurp,\n",
    "        \"aurp_variance\": aurp_variance\n",
    "    }\n",
    "\n",
    "    # 将当前实验的结果添加到结果列表中\n",
    "    all_results.append(results)\n",
    "\n",
    "# 结果保存路径\n",
    "csv_file = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/ML_models/eight_sample_11features_test/5fold_features_ablation/cross_ablation_results.csv'\n",
    "\n",
    "# 检查文件是否存在\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# 打开文件并追加结果\n",
    "with open(csv_file, mode='a', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=results.keys())\n",
    "    \n",
    "    # 如果文件不存在，写入标题\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # 写入每次实验的结果\n",
    "    for result in all_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(\"Experiment completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with features_num = 640\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 600\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 560\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 520\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 480\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 440\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 400\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 360\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 320\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 280\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 240\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 200\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 160\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 120\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 80\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Running with features_num = 40\n",
      "Training fold 1\n",
      "Training fold 2\n",
      "Training fold 3\n",
      "Training fold 4\n",
      "Training fold 5\n",
      "Experiment completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# 将所有特征顺序倒序消融，先消除coverage\n",
    "label_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/labels/'\n",
    "sample_folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/samples/'\n",
    "# 定义不同的 features_num\n",
    "features_nums = [640, 600, 560, 520, 480, 440, 400, 360, 320, 280, 240, 200, 160, 120, 80, 40]\n",
    "# features_nums = [600]\n",
    "# 结果记录\n",
    "all_results = []\n",
    "\n",
    "label_csv_files = sorted([f for f in os.listdir(label_folder_path) if f.endswith('.csv')])\n",
    "sample_csv_files = sorted([f for f in os.listdir(sample_folder_path) if f.endswith('.csv')])\n",
    "\n",
    "# 用于存储所有标签和样本的 DataFrame\n",
    "all_labels = []\n",
    "all_samples = []\n",
    "\n",
    "# 加载所有标签和样本文件\n",
    "for label_file, sample_file in zip(label_csv_files, sample_csv_files):\n",
    "    all_labels.append(pd.read_csv(os.path.join(label_folder_path, label_file)))\n",
    "    all_samples.append(pd.read_csv(os.path.join(sample_folder_path, sample_file)))\n",
    "\n",
    "# 合并所有标签和样本\n",
    "all_labels_df = pd.concat(all_labels, axis=0)\n",
    "all_samples_df = pd.concat(all_samples, axis=0)\n",
    "\n",
    "# 合并样本和标签数据\n",
    "merged_df = pd.merge(all_samples_df, all_labels_df, on='sample', how='left')\n",
    "\n",
    "# 调用 equal_kind 函数处理合并后的数据\n",
    "merged_df = equal_kind(merged_df)\n",
    "# 将特征列倒序\n",
    "first_col = merged_df.iloc[:, 0]\n",
    "last_col = merged_df.iloc[:, -1]\n",
    "middle_cols_reversed = merged_df.iloc[:, 1:-1].iloc[:, ::-1]\n",
    "merged_df_reordered = pd.concat([first_col, middle_cols_reversed, last_col], axis=1)\n",
    "\n",
    "\n",
    "for features_num in features_nums:\n",
    "    print(f\"Running with features_num = {features_num}\")\n",
    "    \n",
    "    # 准备数据集\n",
    "    main_features = merged_df_reordered.iloc[:, 1:features_num + 1].values  # 提取特征列\n",
    "    main_labels = merged_df_reordered.iloc[:, -1].values  # 提取标签列\n",
    "    # 初始化模型\n",
    "    model = CrossAttentionModel(input_dim=features_num // 40, seq_len=40, hidden_dim=64, dropout_rate=0.15)\n",
    "    \n",
    "    # 训练参数\n",
    "    num_epochs = 60\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "\n",
    "    # 五折交叉验证\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_auroc = []\n",
    "    fold_F1 = []\n",
    "    fold_aurp = []\n",
    "\n",
    "    # 五折交叉验证\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(main_features)):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        \n",
    "        # 获取当前折的训练集和验证集\n",
    "        X_train, X_val = main_features[train_index], main_features[val_index]\n",
    "        y_train, y_val = main_labels[train_index], main_labels[val_index]\n",
    "\n",
    "        # 创建训练和验证数据集\n",
    "        train_dataset = CustomDataset(X_train, y_train, input_dim=features_num // 40, seq_len=40)\n",
    "        val_dataset = CustomDataset(X_val, y_val, input_dim=features_num // 40, seq_len=40)\n",
    "\n",
    "        # 创建训练和验证数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 初始化模型并优化器\n",
    "        model._initialize_weights()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # 在当前折上进行训练和验证\n",
    "        acc, auroc, F1, aurp = train_ch7(model, train_iter=train_loader, val_iter=val_loader, loss=loss_fn, num_epochs=num_epochs, updater=optimizer, checkpoint_dir=f'/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/ML_models/eight_sample_11features_test/DL_saved/single_column_11f_test0.1_findauc/fold{fold+1}/')\n",
    "        \n",
    "        fold_accuracies.append(acc)\n",
    "        fold_auroc.append(auroc)\n",
    "        fold_F1.append(F1)\n",
    "        fold_aurp.append(aurp)\n",
    "\n",
    "    # 计算五折的平均值和方差\n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    acc_variance = np.var(fold_accuracies)\n",
    "    mean_auroc = np.mean(fold_auroc)\n",
    "    auroc_variance = np.var(fold_auroc)\n",
    "    mean_F1  = np.mean(fold_F1)\n",
    "    F1_variance = np.var(fold_F1)\n",
    "    mean_aurp = np.mean(fold_aurp)\n",
    "    aurp_variance = np.var(fold_aurp)\n",
    "\n",
    "    # 设置当前实验的结果\n",
    "    results = {\n",
    "        \"features_num\": features_num,\n",
    "        \"mean_accuracy\": mean_acc,\n",
    "        \"accuracy_variance\": acc_variance,\n",
    "        \"mean_auroc\": mean_auroc,\n",
    "        \"auroc_variance\": auroc_variance,\n",
    "        \"mean_F1\": mean_F1,\n",
    "        \"F1_variance\": F1_variance,\n",
    "        \"mean_aurp\": mean_aurp,\n",
    "        \"aurp_variance\": aurp_variance\n",
    "    }\n",
    "\n",
    "    # 将当前实验的结果添加到结果列表中\n",
    "    all_results.append(results)\n",
    "\n",
    "# 结果保存路径\n",
    "csv_file = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/ML_models/eight_sample_11features_test/5fold_features_ablation/cross_ablation_reveresd_results.csv'\n",
    "\n",
    "# 检查文件是否存在\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# 打开文件并追加结果\n",
    "with open(csv_file, mode='a', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=results.keys())\n",
    "    \n",
    "    # 如果文件不存在，写入标题\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # 写入每次实验的结果\n",
    "    for result in all_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(\"Experiment completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
