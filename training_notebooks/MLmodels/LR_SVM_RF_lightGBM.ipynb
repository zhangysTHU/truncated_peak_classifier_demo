{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python import\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn import svm\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance of model\n",
    "def evaluate_performance(y_test, y_pred, y_prob):\n",
    "    # AUROC\n",
    "    auroc = metrics.roc_auc_score(y_test,y_prob)\n",
    "    auroc_curve = metrics.roc_curve(y_test, y_prob)\n",
    "    # AUPRC\n",
    "    auprc=metrics.average_precision_score(y_test, y_prob) \n",
    "    auprc_curve=metrics.precision_recall_curve(y_test, y_prob)\n",
    "    #Accuracy\n",
    "    accuracy=metrics.accuracy_score(y_test,y_pred) \n",
    "    #MCC\n",
    "    mcc=metrics.matthews_corrcoef(y_test,y_pred)\n",
    "    \n",
    "    recall=metrics.recall_score(y_test, y_pred)\n",
    "    precision=metrics.precision_score(y_test, y_pred)\n",
    "    f1=metrics.f1_score(y_test, y_pred)\n",
    "    class_report=metrics.classification_report(y_test, y_pred,target_names = [\"control\",\"case\"])\n",
    "\n",
    "    model_perf = {\"auroc\":auroc,\"auroc_curve\":auroc_curve,\n",
    "                  \"auprc\":auprc,\"auprc_curve\":auprc_curve,\n",
    "                  \"accuracy\":accuracy, \"mcc\": mcc,\n",
    "                  \"recall\":recall,\"precision\":precision,\"f1\":f1,\n",
    "                  \"class_report\":class_report}\n",
    "        \n",
    "    return model_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output result of evaluation\n",
    "def eval_output(model_perf,path):\n",
    "    with open(os.path.join(path,\"Evaluate_Result_TestSet.txt\"),'w') as f:\n",
    "        f.write(\"AUROC=%s\\tAUPRC=%s\\tAccuracy=%s\\tMCC=%s\\tRecall=%s\\tPrecision=%s\\tf1_score=%s\\n\" %\n",
    "               (model_perf[\"auroc\"],model_perf[\"auprc\"],model_perf[\"accuracy\"],model_perf[\"mcc\"],model_perf[\"recall\"],model_perf[\"precision\"],model_perf[\"f1\"]))\n",
    "        f.write(\"\\n######NOTE#######\\n\")\n",
    "        f.write(\"#According to help_documentation of sklearn.metrics.classification_report:in binary classification, recall of the positive class is also known as sensitivity; recall of the negative class is specificity#\\n\\n\")\n",
    "        f.write(model_perf[\"class_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUROC of model\n",
    "def plot_AUROC(model_perf,path):\n",
    "    #get AUROC,FPR,TPR and threshold\n",
    "    roc_auc = model_perf[\"auroc\"]\n",
    "    fpr,tpr,threshold = model_perf[\"auroc_curve\"]\n",
    "    #return AUROC info\n",
    "    temp_df = pd.DataFrame({\"FPR\":fpr,\"TPR\":tpr})\n",
    "    temp_df.to_csv(os.path.join(path,\"AUROC_info.txt\"),header = True,index = False, sep = '\\t')\n",
    "    #plot\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='AUROC (area = %0.2f)' % roc_auc) \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"AUROC of Models\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(path,\"AUROC_TestSet.pdf\"),format = \"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "SEED = 100\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"./ML_Model_Output\"\n",
    "if not (os.path.exists(output_dir)):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有样本\n",
    "folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/labels/'\n",
    "\n",
    "# 获取该文件夹下所有 CSV 文件的文件名，并按文件名顺序排序\n",
    "csv_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.csv')])\n",
    "\n",
    "# 创建一个空的列表，用于存储所有的 DataFrame\n",
    "dfs = []\n",
    "\n",
    "# 依次读取每个 CSV 文件并将其添加到列表中\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)  # 读取 CSV 文件\n",
    "    dfs.append(df)  # 将 DataFrame 添加到列表中\n",
    "\n",
    "# 将所有的 DataFrame 按列合并\n",
    "labels = pd.concat(dfs, axis=0)\n",
    "\n",
    "# 设置文件夹路径\n",
    "folder_path = '/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/NomalSamples/samples/'\n",
    "# 获取该文件夹下所有 CSV 文件的文件名，并按文件名顺序排序\n",
    "csv_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.csv')])\n",
    "# 创建一个空的列表，用于存储所有的 DataFrame\n",
    "dfs = []\n",
    "# 依次读取每个 CSV 文件并将其添加到列表中\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)  # 读取 CSV 文件\n",
    "    dfs.append(df)  # 将 DataFrame 添加到列表中\n",
    "# 将所有的 DataFrame 按列合并\n",
    "dataset = pd.concat(dfs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载两个数据集，对样本数较多的数据集进行下采样平衡\n",
    "#dataset = pd.read_csv('/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/SRR11004118_sample_prepared.csv')    \n",
    "#labels = pd.read_csv('/BioII/lulab_b/huangkeyun/zhangys/alkb-seq/resources/SRR11004118_labels.csv')\n",
    "dataset = pd.concat([dataset, labels['label']], axis=1)\n",
    "df_y0 = dataset[dataset['label'] == 0]\n",
    "df_y1 = dataset[dataset['label'] == 1]\n",
    "\n",
    "# 确定两个子集中数量较少的那个\n",
    "min_count = min(len(df_y0), len(df_y1))\n",
    "\n",
    "# 从两个子集中随机选择等量的样本\n",
    "df_y0_balanced = df_y0.sample(n=min_count, random_state=42) if len(df_y0) > min_count else df_y0\n",
    "df_y1_balanced = df_y1.sample(n=min_count, random_state=42) if len(df_y1) > min_count else df_y1\n",
    "# 合并这两个平衡后的子集\n",
    "balanced_df = pd.concat([df_y0_balanced, df_y1_balanced])\n",
    "# 打乱合并后的数据集的顺序\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "output_dir = './ML_Model_Output'\n",
    "\n",
    "X = balanced_df.iloc[:, 1:-1].values\n",
    "y = balanced_df.iloc[:, -1].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Construct Logistic Regression model\n",
    "print(\"\\n*** Logistic Regression  ***\")\n",
    "\n",
    "# Logistic Regression params\n",
    "lr_param_dict = {\n",
    "    \"penalty\":[\"l2\"],\n",
    "    \"C\":[1e-3, 5e-3, 1e-2, 0.05, 0.1, 0.5,1,5,10,50,100,500,1000],\n",
    "    \"solver\":[\"liblinear\"],\n",
    "    \"random_state\":[SEED]\n",
    "}\n",
    "\n",
    "#Initiate model\n",
    "lr_model = LogisticRegression()\n",
    "#Adjust hyper-parameters with 5-fold cross validation\n",
    "lr_rscv = RandomizedSearchCV(lr_model, lr_param_dict, n_iter=100,cv = 5,verbose = 0,\n",
    "                          scoring = \"roc_auc\",random_state=SEED,n_jobs = 30)\n",
    "lr_rscv.fit(x_train, y_train)  \n",
    "\n",
    "#Evaluate best Lasso model\n",
    "#Output path\n",
    "path = os.path.join(output_dir,\"LogisticRegression\")\n",
    "if not (os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "# Model performance(AUROC) on cross-validation dataset\n",
    "lr_cv_perf = np.array([ lr_rscv.cv_results_[\"split%s_test_score\"%str(i)] for i in range(5)])[:,lr_rscv.best_index_]\n",
    "\n",
    "#Get best model with score [max(mean(auc(5 cross validation)))]\n",
    "lr_best_model = lr_rscv.best_estimator_\n",
    "#Get predict_class(y_pred) and predict_probality_for_case(y_prob) of TestSet\n",
    "y_pred = lr_best_model.predict(x_test)\n",
    "y_prob = lr_best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "#Get model performance\n",
    "model_perf = evaluate_performance(y_test,y_pred,y_prob)\n",
    "#Output result of evaluation\n",
    "eval_output(model_perf,path)\n",
    "#You can make bar plot consisted of accuracy,sensitivity,specificity,auroc,f1 score,MCC,precision,recall,auprc according to the \"Evaluate_Result_TestSet.txt\"\n",
    "# Plot AUROC\n",
    "plot_AUROC(model_perf,path)\n",
    "\n",
    "#save model\n",
    "joblib.dump(lr_best_model,os.path.join(path,\"best_LogisticRegression_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Construct SVM model\n",
    "print(\"\\n*** SVM ***\")\n",
    "\n",
    "# SVM params\n",
    "SVM_param_dict = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[0.01,0.1,1,10, 100], \n",
    "    'gamma':[0.001, 0.005, 0.1 ,0.5,1, 2],\n",
    "    \"probability\":[True],\n",
    "    \"random_state\":[SEED]\n",
    "}\n",
    "\n",
    "#Initiate model\n",
    "SVM_model = svm.SVC()\n",
    "#Adjust hyper-parameters with 5-fold cross validation\n",
    "SVM_rscv = RandomizedSearchCV(SVM_model, SVM_param_dict, n_iter=100,cv = 5,verbose = 0,\n",
    "                          scoring = \"roc_auc\",random_state=SEED,n_jobs =30)\n",
    "SVM_rscv.fit(x_train, y_train) \n",
    "\n",
    "\n",
    "#Evaluate best SVM model\n",
    "#Output path\n",
    "path = os.path.join(output_dir,\"SVM\")\n",
    "if not (os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "# Model performance(AUROC) on cross-validation dataset\n",
    "SVM_cv_perf = np.array([ SVM_rscv.cv_results_[\"split%s_test_score\"%str(i)] for i in range(5)])[:,SVM_rscv.best_index_]\n",
    "\n",
    "#Get best model with score [max(mean(auc(5 cross validation)))]\n",
    "svm_best_model = SVM_rscv.best_estimator_\n",
    "#Get predict_class(y_pred) and predict_probality_for_case(y_prob) of TestSet\n",
    "y_pred = svm_best_model.predict(x_test)\n",
    "y_prob = svm_best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "#Get model performance\n",
    "model_perf = evaluate_performance(y_test,y_pred,y_prob)\n",
    "#Output result of evaluation\n",
    "eval_output(model_perf,path)\n",
    "#You can make bar plot consisted of accuracy,sensitivity,specificity,auroc,f1 score,MCC,precision,recall,auprc according to the \"Evaluate_Result_TestSet.txt\"\n",
    "# Plot AUROC\n",
    "plot_AUROC(model_perf,path)\n",
    "\n",
    "#save model\n",
    "joblib.dump(svm_best_model,os.path.join(path,\"best_SVM_model.pkl\"))\n",
    "#load model\n",
    "#svm_best_model = joblib.load(os.path.join(path,\"best_SVM_model.pkl\"))\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Construct Random Forest model\n",
    "print(\"\\n*** Random Forest  ***\")\n",
    "\n",
    "# Random Forest params\n",
    "rf_param_dict = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500, 1000],\n",
    "    \"max_depth\": [None, 10, 20, 50, 100],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": ['auto', 'sqrt', 'log2'],\n",
    "    \"random_state\": [SEED]\n",
    "}\n",
    "\n",
    "# Initiate model\n",
    "rf_model = RandomForestClassifier()\n",
    "# Adjust hyper-parameters with 5-fold cross-validation\n",
    "rf_rscv = RandomizedSearchCV(rf_model, rf_param_dict, n_iter=100, cv=5, verbose=0,\n",
    "                             scoring=\"roc_auc\", random_state=SEED, n_jobs=30)\n",
    "rf_rscv.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate best Random Forest model\n",
    "# Output path\n",
    "path = os.path.join(output_dir, \"RandomForest\")\n",
    "if not (os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    "\n",
    "# Model performance(AUROC) on cross-validation dataset\n",
    "rf_cv_perf = np.array([rf_rscv.cv_results_[\"split%s_test_score\" % str(i)] for i in range(5)])[:, rf_rscv.best_index_]\n",
    "\n",
    "# Get best model with score [max(mean(auc(5 cross validation)))]\n",
    "rf_best_model = rf_rscv.best_estimator_\n",
    "# Get predict_class(y_pred) and predict_probability_for_case(y_prob) of TestSet\n",
    "y_pred = rf_best_model.predict(x_test)\n",
    "y_prob = rf_best_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Get model performance\n",
    "model_perf = evaluate_performance(y_test, y_pred, y_prob)\n",
    "# Output result of evaluation\n",
    "eval_output(model_perf, path)\n",
    "# You can make bar plot consisted of accuracy, sensitivity, specificity, auroc, f1 score, MCC, precision, recall, auprc according to the \"Evaluate_Result_TestSet.txt\"\n",
    "# Plot AUROC\n",
    "plot_AUROC(model_perf, path)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_best_model, os.path.join(path, \"best_RandomForest_model.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Construct Random Forest model\n",
    "print(\"\\n*** Random Forest  ***\")\n",
    "\n",
    "# Random Forest params\n",
    "rf_param_dict = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500, 1000],\n",
    "    \"max_depth\": [None, 10, 20, 50, 100],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": ['auto', 'sqrt', 'log2'],\n",
    "    \"random_state\": [SEED]\n",
    "}\n",
    "\n",
    "# Initiate model\n",
    "rf_model = RandomForestClassifier()\n",
    "# Adjust hyper-parameters with 5-fold cross-validation\n",
    "rf_rscv = RandomizedSearchCV(rf_model, rf_param_dict, n_iter=1000, cv=5, verbose=0,\n",
    "                             scoring=\"roc_auc\", random_state=SEED, n_jobs=30)\n",
    "rf_rscv.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate best Random Forest model\n",
    "# Output path\n",
    "path = os.path.join(output_dir, \"RandomForest\")\n",
    "if not (os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    "\n",
    "# Model performance(AUROC) on cross-validation dataset\n",
    "rf_cv_perf = np.array([rf_rscv.cv_results_[\"split%s_test_score\" % str(i)] for i in range(5)])[:, rf_rscv.best_index_]\n",
    "\n",
    "# Get best model with score [max(mean(auc(5 cross validation)))]\n",
    "rf_best_model = rf_rscv.best_estimator_\n",
    "# Get predict_class(y_pred) and predict_probability_for_case(y_prob) of TestSet\n",
    "y_pred = rf_best_model.predict(x_test)\n",
    "y_prob = rf_best_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Get model performance\n",
    "model_perf = evaluate_performance(y_test, y_pred, y_prob)\n",
    "# Output result of evaluation\n",
    "eval_output(model_perf, path)\n",
    "# You can make bar plot consisted of accuracy, sensitivity, specificity, auroc, f1 score, MCC, precision, recall, auprc according to the \"Evaluate_Result_TestSet.txt\"\n",
    "# Plot AUROC\n",
    "plot_AUROC(model_perf, path)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_best_model, os.path.join(path, \"best_RandomForest_model.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"\\n*** LightGBM ***\")\n",
    "\n",
    "# LightGBM params\n",
    "lgb_param_dict = {\n",
    "    \"learning_rate\":[0.1, 0.05, 0.02, 0.01],\n",
    "    \"num_leaves\": range(10,36,5),\n",
    "    \"max_depth\" : [2,3,4,5,10,20,40,50],\n",
    "    \"min_child_samples\": range(1, 45, 2),\n",
    "    \"colsample_bytree\" : [i / 10 for i in range(2,11)],\n",
    "    \"metric\" : [\"binary_logloss\"],\n",
    "    \"n_jobs\":[1],\n",
    "    \"n_estimators\" : range(100,2500,100),\n",
    "    \"subsample\" :  [i / 10 for i in range(2, 11)],\n",
    "    \"subsample_freq\" : [0, 1, 2],\n",
    "    \"reg_alpha\" : [0, 0.001, 0.005, 0.01, 0.1],\n",
    "    \"reg_lambda\" : [0, 0.001, 0.005, 0.01, 0.1],\n",
    "    \"objective\":[\"binary\"],\n",
    "    \"random_state\":[SEED]\n",
    "}\n",
    "\n",
    "#Initiate model\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "#Adjust hyper-parameters with 5-fold cross validation\n",
    "lgb_rscv = RandomizedSearchCV(lgb_model, lgb_param_dict, n_iter=1000,cv = 5,verbose = 0,\n",
    "                          scoring = \"roc_auc\",random_state=SEED,n_jobs = 30)\n",
    "lgb_rscv.fit(x_train, y_train)   \n",
    "\n",
    "\n",
    "#Evaluate best LightGBM model\n",
    "#Output path\n",
    "path = os.path.join(output_dir,\"LightGBM\")\n",
    "if not (os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "# Model performance(AUROC) on cross-validation dataset\n",
    "lgb_cv_perf = np.array([ lgb_rscv.cv_results_[\"split%s_test_score\"%str(i)] for i in range(5)])[:,lgb_rscv.best_index_]\n",
    "\n",
    "#Get best model with score [max(mean(auc(5 cross validation)))]\n",
    "lgb_best_model = lgb_rscv.best_estimator_\n",
    "#Get predict_class(y_pred) and predict_probality_for_case(y_prob) of TestSet\n",
    "y_pred = lgb_best_model.predict(x_test)\n",
    "y_prob = lgb_best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "#Get model performance\n",
    "model_perf = evaluate_performance(y_test,y_pred,y_prob)\n",
    "#Output result of evaluation\n",
    "eval_output(model_perf,path)\n",
    "#You can make bar plot consisted of accuracy,sensitivity,specificity,auroc,f1 score,MCC,precision,recall,auprc according to the \"Evaluate_Result_TestSet.txt\"\n",
    "# Plot AUROC\n",
    "plot_AUROC(model_perf,path)\n",
    "\n",
    "#save model\n",
    "joblib.dump(lgb_best_model,os.path.join(path,\"best_LightGBM_model.pkl\"))\n",
    "#load modelq\n",
    "#lgb_best_model = joblib.load(os.path.join(path,\"best_LightGBM_model.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
